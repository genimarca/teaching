{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de clasificación de Texto\n",
    "\n",
    "Se va a ver un ejemplo propio de la tarea análisis de opiniones, en concreto de clasificaciónd de opiniones a nivel de tuit escrito en español.\n",
    "\n",
    "Requerimientos:\n",
    "- Python 3\n",
    "- NLTK\n",
    "- Numpy\n",
    "- Keras\n",
    "- TensorFlow\n",
    "- Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.svm.classes import LinearSVC\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_TOKENIZER = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "CLASSES = []\n",
    "PATH_CORPUS = \"../data/tass14_general_corpus_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comenzamos con la lectura del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    '''Load the corpus into memory\n",
    "    '''\n",
    "    \n",
    "    ids = []\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    ids_append = ids.append\n",
    "    classes_append = CLASSES.append\n",
    "    labels_append = labels.append\n",
    "    tweets_append = tweets.append\n",
    "    with(open(path, 'r', encoding='utf-8')) as input_file:\n",
    "        own_split = str.split\n",
    "        own_strip = str.strip\n",
    "        input_file.readline()\n",
    "        for buffer in input_file:\n",
    "            buffer_fields = own_split(buffer, ';;;')\n",
    "            ids_append(own_strip(buffer_fields[0]))\n",
    "            label = own_strip(buffer_fields[4])\n",
    "            if(label not in CLASSES):\n",
    "                classes_append(label)\n",
    "            labels_append(CLASSES.index(label))\n",
    "            tweets_append(own_strip(buffer_fields[-1]))\n",
    "    \n",
    "    return(ids, labels, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_file_path = PATH_CORPUS\n",
    "ids, labels, tweets = read_corpus(input_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de semilla aleatoria\n",
    "\n",
    "Se define la semilla aleatoria para que la experimentación sea reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del conjunto de entrenamiento y de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(np.arange(len(tweets)), test_size=0.2, random_state=7)\n",
    "labels_train = [labels[tweet_index] for tweet_index in train_index]\n",
    "labels_test = [labels[tweet_index] for tweet_index in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código de clasificación\n",
    "Se van a comparar 3 algoritmos:\n",
    "- SVM\n",
    "- RNN basada en la célula LSTM, en el que los tuits están represenatdos por valores TF-IDF.\n",
    "- RNN basada en la célula LSTM, en el los tuits están representados por vectores de palabras (word embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize an input text\n",
    "    \n",
    "    Args:\n",
    "        text: A String with the text to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        A list of Strings (tokens)\n",
    "    \"\"\"\n",
    "    text_tokenized = TWEET_TOKENIZER.tokenize(text)\n",
    "    return text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_performamnce(y_labels, y_classified_labels, model_name):\n",
    "    \n",
    "    classes_index = [CLASSES.index(c) for c in CLASSES]\n",
    "    accruacy = metrics.accuracy_score(y_labels, y_classified_labels)\n",
    "    macro_precision = metrics.precision_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_recall = metrics.recall_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    macro_f1 = metrics.f1_score(y_labels, y_classified_labels, labels=classes_index, average=\"macro\")\n",
    "    \n",
    "    print(\"\\n*** Results \" + model_name + \"***\")\n",
    "    print(\"Macro-Precision: \" + str(macro_precision))\n",
    "    print(\"Macro-Recall: \" + str(macro_recall))\n",
    "    print(\"Macro-F1: \" + str(macro_f1))\n",
    "    print(\"Accuracy: \" + str(accruacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_linear_svm(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classifies using SVM as classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    \n",
    "    classifier = LinearSVC(multi_class=\"ovr\", random_state=random_state)\n",
    "    print(\"Start SVM training\")\n",
    "    classifier = classifier.fit(train_sparse_matrix_features_tfidf, labels_train)\n",
    "    print(\"Finish SVM training\")\n",
    "    y_labels = classifier.predict(test_sparse_matrix_features_tfidf)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_svn = classification_linear_svm(tweets, train_index, test_index, labels_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_quality_performamnce(labels_test, y_labels_svn, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_tfidf_rnn(tweets, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification using a RNN with tfidf as features\n",
    "    \"\"\"\n",
    "    #Representation\n",
    "    tfidf_parser = TfidfVectorizer(tokenizer=tokenize, lowercase=False, analyzer='word')\n",
    "    tweets_train = [tweets[tweet_index] for tweet_index in train_index]\n",
    "    tweets_test = [tweets[tweet_index] for tweet_index in test_index]\n",
    "    \n",
    "    train_sparse_matrix_features_tfidf = tfidf_parser.fit_transform(tweets_train)\n",
    "    test_sparse_matrix_features_tfidf = tfidf_parser.transform(tweets_test)\n",
    "    \n",
    "    train_features_tfidf = []\n",
    "    own_train_features_tfidf_append = train_features_tfidf.append\n",
    "    lengths_tweets = []\n",
    "    own_lengths_tweets_append = lengths_tweets.append\n",
    "    \n",
    "    for tweet in train_sparse_matrix_features_tfidf:\n",
    "        own_train_features_tfidf_append(tweet.data)\n",
    "        own_lengths_tweets_append(len(tweet.data))\n",
    "    \n",
    "\n",
    "    test_features_tfidf = [tweet.data for tweet in test_sparse_matrix_features_tfidf]\n",
    "    #Average length\n",
    "    max_len_input = int(np.average(lengths_tweets, 0))\n",
    "    #NN model\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(LSTM(32, input_shape=(max_len_input,1)))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    train_features_tfidf_pad = sequence.pad_sequences(train_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(train_features_tfidf[0][0]))\n",
    "    train_features_tfidf_pad = np.expand_dims(train_features_tfidf_pad, axis=-1)\n",
    "    print(\"Start RNN LSTM training\")\n",
    "    nn_model.fit(train_features_tfidf_pad, labels_train, batch_size=32, epochs=15, verbose=1)\n",
    "    print(\"Finish RNN LSTM training\")\n",
    "    test_features_tfidf_pad = sequence.pad_sequences(test_features_tfidf, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(test_features_tfidf[0][0]))\n",
    "    test_features_tfidf_pad = np.expand_dims(test_features_tfidf_pad, axis=-1)\n",
    "    y_labels = nn_model.predict_classes(test_features_tfidf_pad, batch_size=32, verbose=1)\n",
    "    \n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_rnn = classification_tfidf_rnn(tweets, train_index, test_index, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_quality_performamnce(labels_test, y_labels_rnn, \"RNN_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_vocabulary(corpus):\n",
    "    \"\"\"Creates the vocabulary of the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list os str (documents)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple whose first element is a dictionary word-index and the second\n",
    "        element is a list of list in which each position is the index of the \n",
    "        token in the vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = {}\n",
    "    corpus_indexes = []\n",
    "    index = 1\n",
    "    for doc in corpus:\n",
    "        doc_indexes = []\n",
    "        tokens = tokenize(doc)\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = index\n",
    "                doc_indexes.append(index)\n",
    "                index += 1\n",
    "            else:\n",
    "                doc_indexes.append(vocabulary[token])\n",
    "        \n",
    "            \n",
    "        corpus_indexes.append(doc_indexes)\n",
    "    return (vocabulary, corpus_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation_embedings_rnn(corpus, train_index, test_index, labels_train, random_state=None):\n",
    "    \"\"\"Classification with RNN and embedings (no pre-trained)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Build vocabulary and corpus indexes\n",
    "    vocabulary, tweets_index = fit_transform_vocabulary(corpus)\n",
    "    \n",
    "    corpus_train = []\n",
    "    own_corpus_train_append = corpus_train.append\n",
    "    lengths_tweets = []\n",
    "    own_lengths_tweets_append = lengths_tweets.append\n",
    "    p = []\n",
    "    for t_index in train_index:\n",
    "        own_corpus_train_append(tweets_index[t_index])\n",
    "        own_lengths_tweets_append(len(tweets_index[t_index]))\n",
    "        p.append(corpus[t_index])\n",
    "    max_len_input = int(np.average(lengths_tweets, 0))\n",
    "    corpus_test = [tweets_index[t_index] for t_index in test_index]\n",
    "    \n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Embedding(len(vocabulary)+1, 32, input_length=max_len_input))\n",
    "    nn_model.add(LSTM(32))\n",
    "    nn_model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "    nn_model.compile(optimizer=\"adam\", \n",
    "                     loss=\"sparse_categorical_crossentropy\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "    \n",
    "    \n",
    "    train_features_tfidf_pad = sequence.pad_sequences(corpus_train, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train[0][0]))\n",
    "    \n",
    "    \n",
    "    print(\"Start RNN EMBEDDING LSTM training\")\n",
    "    nn_model.fit(train_features_tfidf_pad, labels_train, batch_size=32, epochs=15, verbose=1)\n",
    "    print(\"Finish RNN EMBEDDING LSTM training\")\n",
    "    test_features_tfidf_pad = sequence.pad_sequences(corpus_test, maxlen=max_len_input, padding=\"post\", truncating=\"post\", dtype=type(corpus_train[0][0]))\n",
    "    y_labels = nn_model.predict_classes(test_features_tfidf_pad, batch_size=32, verbose=1)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_embeddings_rnn = classifcation_embedings_rnn(tweets, train_index, test_index, labels_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_quality_performamnce(labels_test, y_labels_embeddings_rnn, \"RNN_EMBEDINGS_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
